{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get some proteins sequences from one of jeppe's data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import imp\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_id_dict = {'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, 'I': 7,\n",
    "              'K': 8, 'L': 9, 'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14, \n",
    "              'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa_to_onehot(aa_str, aa_to_nr, mask=None):\n",
    "    if mask!=None:\n",
    "        mask_ind = np.asarray([x=='+' for x in mask])*1\n",
    "        mask_ind = np.nonzero(mask_ind)\n",
    "        aa_str = \"\".join([aa_str[x] for x in mask_ind[0]]) # because it gets put in another list\n",
    "    init_array = np.zeros( (len(aa_to_nr.keys()), len(aa_str)) )\n",
    "    for i,j in enumerate(aa_str):\n",
    "        init_array[aa_to_nr[j], i] = 1\n",
    "    return(init_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_proteinnet_file(file, stop_at=1000):\n",
    "    protein_dict = {}\n",
    "    with open(file) as input:\n",
    "        lines = input.readlines()\n",
    "        curr_id = None\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if line == '[ID]':\n",
    "                curr_id = lines[i+1].strip()\n",
    "                protein_dict[curr_id] = {}\n",
    "                protein_dict[curr_id]['primary'] = lines[i+3].strip()\n",
    "            if line == '[TERTIARY]':\n",
    "                coords = []\n",
    "                for j in range(3):\n",
    "                    coords.append(np.fromstring(lines[i+j+1], sep='\\t'))\n",
    "                protein_dict[curr_id]['tertiary'] = np.array(coords)\n",
    "            if line == '[MASK]':\n",
    "                protein_dict[curr_id]['mask'] = lines[i+1].strip()\n",
    "            if len(protein_dict.keys()) >= stop_at:\n",
    "                break\n",
    "    filter_seqs(protein_dict)\n",
    "    return(protein_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_coords(coords, mask):\n",
    "    mask = np.array([x=='+' for x in mask])\n",
    "    mask_stretched = np.repeat(mask, 3)\n",
    "    coords_filt = coords[:, mask_stretched]\n",
    "    return(coords_filt)\n",
    "\n",
    "def filter_seqs(protein_dict):\n",
    "    re_chainbreak = re.compile(\"\\-*\\+*\\+\\-+\\+\\+*\\-*\")\n",
    "    keys_to_remove = {}\n",
    "    for key in protein_dict.keys():\n",
    "        if len(protein_dict[key].keys()) < 3:\n",
    "            keys_to_remove[key] = ' missing structure, removing...'\n",
    "            continue\n",
    "        mask = protein_dict[key]['mask']\n",
    "        if re_chainbreak.search(mask):\n",
    "            keys_to_remove[key] = \" has a chainbreak, removing...\"\n",
    "        else:\n",
    "            coords = protein_dict[key]['tertiary']\n",
    "            coords_filt = filter_coords(coords, mask)\n",
    "            protein_dict[key]['tertiary'] = coords_filt\n",
    "    for key in keys_to_remove.keys():\n",
    "        print(key, keys_to_remove[key])\n",
    "        del protein_dict[key]\n",
    "\n",
    "def new_dihedral(p0, p1, p2, p3): # COPY PASTA'D FROM STACKEXCHANGE\n",
    "    \"\"\"Praxeolitic formula\n",
    "    1 sqrt, 1 cross product\"\"\"\n",
    "\n",
    "    b0 = -1.0*(p1 - p0)\n",
    "    b1 = p2 - p1\n",
    "    b2 = p3 - p2\n",
    "\n",
    "    # normalize b1 so that it does not influence magnitude of vector\n",
    "    # rejections that come next\n",
    "    b1 /= np.linalg.norm(b1)\n",
    "\n",
    "    # vector rejections\n",
    "    # v = projection of b0 onto plane perpendicular to b1\n",
    "    #   = b0 minus component that aligns with b1\n",
    "    # w = projection of b2 onto plane perpendicular to b1\n",
    "    #   = b2 minus component that aligns with b1\n",
    "    v = b0 - np.dot(b0, b1)*b1\n",
    "    w = b2 - np.dot(b2, b1)*b1\n",
    "\n",
    "    # angle between v and w in a plane is the torsion angle\n",
    "    # v and w may not be normalized but that's fine since tan is y/x\n",
    "    x = np.dot(v, w)\n",
    "    y = np.dot(np.cross(b1, v), w)\n",
    "    #return np.degrees(np.arctan2(y, x))\n",
    "    return np.arctan2(y, x)\n",
    "\n",
    "def calc_angles(coords):\n",
    "    N = coords.shape[1]\n",
    "    angles_all = []\n",
    "    for i in range(0, N-3, 3):\n",
    "        psi = new_dihedral(*[coords[:, x] for x in range(i, i+4)])\n",
    "        omega = new_dihedral(*[coords[:, x] for x in range(i+1, i+5)])\n",
    "        phi = new_dihedral(*[coords[:, x] for x in range(i+2, i+6)])\n",
    "        angles = [psi, omega, phi]\n",
    "        angles_all.append(angles)\n",
    "    return(np.array(angles_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2G6Y_1_A  has a chainbreak, removing...\n",
      "1NNX_1_A  has a chainbreak, removing...\n",
      "1FL1_1_A  has a chainbreak, removing...\n",
      "1OYS_1_A  has a chainbreak, removing...\n",
      "1YFS_d1yfsb1  has a chainbreak, removing...\n",
      "1ZNN_d1znnf-  has a chainbreak, removing...\n",
      "1DL3_1_A  has a chainbreak, removing...\n",
      "1EMS_1_A  has a chainbreak, removing...\n",
      "1YSR_1_A  has a chainbreak, removing...\n",
      "1TFF_1_A  has a chainbreak, removing...\n",
      "1V39_1_A  has a chainbreak, removing...\n",
      "1X2G_1_A  has a chainbreak, removing...\n",
      "1ZAO_d1zaoa2  has a chainbreak, removing...\n",
      "2FIY_1_A  has a chainbreak, removing...\n",
      "1HYV_1_A  has a chainbreak, removing...\n",
      "1JDP_1_A  has a chainbreak, removing...\n",
      "1X7F_d1x7fa2  has a chainbreak, removing...\n",
      "1D9Z_1_A  has a chainbreak, removing...\n",
      "1NH9_1_A  has a chainbreak, removing...\n",
      "1UN8_d1un8b1  has a chainbreak, removing...\n",
      "1I7G_1_A  has a chainbreak, removing...\n",
      "1AU7_3_A  has a chainbreak, removing...\n",
      "1MZB_1_A  has a chainbreak, removing...\n",
      "1FOT_1_A  has a chainbreak, removing...\n",
      "1SKQ_1_A  has a chainbreak, removing...\n",
      "1US3_1_A  has a chainbreak, removing...\n",
      "1O97_2_D  has a chainbreak, removing...\n",
      "1MUC_d1mucb2  has a chainbreak, removing...\n",
      "1EPU_1_A  has a chainbreak, removing...\n",
      "1MZU_1_A  has a chainbreak, removing...\n",
      "2FH5_2_B  has a chainbreak, removing...\n",
      "1GV0_1_A  has a chainbreak, removing...\n",
      "1VKP_1_A  has a chainbreak, removing...\n",
      "1RWI_1_B  has a chainbreak, removing...\n",
      "2F2U_1_A  has a chainbreak, removing...\n",
      "1J5X_1_A  has a chainbreak, removing...\n",
      "1URH_d1urhb2  has a chainbreak, removing...\n",
      "1R8J_1_A  has a chainbreak, removing...\n",
      "1DV1_1_A  has a chainbreak, removing...\n",
      "1YDM_1_A  has a chainbreak, removing...\n",
      "1H8E_5_I  has a chainbreak, removing...\n",
      "2AQL_1_A  has a chainbreak, removing...\n",
      "2SHP_d2shpb1  has a chainbreak, removing...\n",
      "1S0U_d1s0ua3  has a chainbreak, removing...\n",
      "2AHO_d2ahob2  has a chainbreak, removing...\n",
      "1EKE_1_A  has a chainbreak, removing...\n",
      "1TW9_d1tw9h1  has a chainbreak, removing...\n",
      "1YRR_1_A  has a chainbreak, removing...\n",
      "1JAL_d1jalb1  has a chainbreak, removing...\n",
      "1XKK_1_A  has a chainbreak, removing...\n",
      "1Z0X_1_A  has a chainbreak, removing...\n",
      "1A7S_1_A  has a chainbreak, removing...\n",
      "1PQ1_1_A  has a chainbreak, removing...\n",
      "2SHP_d2shpb3  has a chainbreak, removing...\n",
      "1VLR_1_A  has a chainbreak, removing...\n",
      "1GML_1_A  has a chainbreak, removing...\n",
      "2C46_1_A  has a chainbreak, removing...\n",
      "1LKX_1_A  has a chainbreak, removing...\n",
      "1RM1_5_C  has a chainbreak, removing...\n",
      "1YVR_1_A  has a chainbreak, removing...\n",
      "1EX0_1_A  has a chainbreak, removing...\n",
      "1Z7M_2_E  has a chainbreak, removing...\n",
      "1PJR_d1pjra2  has a chainbreak, removing...\n",
      "2CW5_1_A  has a chainbreak, removing...\n",
      "1ES6_d1es6a2  has a chainbreak, removing...\n",
      "1K8K_1_A  has a chainbreak, removing...\n",
      "1W36_1_B  has a chainbreak, removing...\n",
      "1VPX_1_A  has a chainbreak, removing...\n",
      "1IQV_1_A  has a chainbreak, removing...\n",
      "1XM9_1_A  has a chainbreak, removing...\n",
      "1EKQ_1_A  has a chainbreak, removing...\n",
      "2FM9_1_A  has a chainbreak, removing...\n",
      "1JMA_1_B  has a chainbreak, removing...\n",
      "1QF8_1_A  has a chainbreak, removing...\n",
      "1GTE_d1gted2  has a chainbreak, removing...\n",
      "1RPY_1_A  has a chainbreak, removing...\n",
      "1F7C_1_A  has a chainbreak, removing...\n",
      "1WRU_1_A  has a chainbreak, removing...\n",
      "2BSY_1_A  has a chainbreak, removing...\n",
      "2BHV_1_A  has a chainbreak, removing...\n",
      "1WOV_1_A  has a chainbreak, removing...\n",
      "1Q9C_1_A  has a chainbreak, removing...\n",
      "1TB3_1_A  has a chainbreak, removing...\n",
      "1BY2_1_A  has a chainbreak, removing...\n",
      "1QBE_1_A  has a chainbreak, removing...\n",
      "1CD9_d1cd9d2  has a chainbreak, removing...\n",
      "1NT0_d1nt0g1  has a chainbreak, removing...\n",
      "1RQ2_d1rq2b1  has a chainbreak, removing...\n",
      "1J8Y_d1j8yf2  has a chainbreak, removing...\n",
      "1PD1_d1pd1a1  has a chainbreak, removing...\n",
      "1W6U_1_A  has a chainbreak, removing...\n",
      "1U04_d1u04a3  has a chainbreak, removing...\n",
      "1EXU_d1exua2  has a chainbreak, removing...\n",
      "1L7D_1_A  has a chainbreak, removing...\n",
      "2G0C_1_A  has a chainbreak, removing...\n",
      "1YR2_1_A  has a chainbreak, removing...\n",
      "1VLI_d1vlia2  has a chainbreak, removing...\n",
      "1GLV_1_A  has a chainbreak, removing...\n",
      "2FIT_1_A  has a chainbreak, removing...\n",
      "1UE6_1_A  has a chainbreak, removing...\n",
      "1H99_1_A  has a chainbreak, removing...\n",
      "1F3U_2_B  has a chainbreak, removing...\n",
      "1GPJ_1_A  has a chainbreak, removing...\n",
      "1Q67_1_A  has a chainbreak, removing...\n",
      "1SPX_1_A  has a chainbreak, removing...\n",
      "1M2V_d1m2va4  has a chainbreak, removing...\n",
      "2A8Y_1_A  has a chainbreak, removing...\n",
      "1Z34_1_A  has a chainbreak, removing...\n",
      "1XSQ_1_A  has a chainbreak, removing...\n",
      "2G9Z_1_A  has a chainbreak, removing...\n",
      "1WZU_1_A  has a chainbreak, removing...\n",
      "1TQ4_1_A  has a chainbreak, removing...\n",
      "1V43_1_A  has a chainbreak, removing...\n",
      "1K38_1_A  has a chainbreak, removing...\n",
      "1MAS_1_A  has a chainbreak, removing...\n",
      "1KL7_1_A  has a chainbreak, removing...\n",
      "1E5S_1_A  has a chainbreak, removing...\n",
      "1G16_1_A  has a chainbreak, removing...\n",
      "1N52_1_A  has a chainbreak, removing...\n",
      "1OJ5_1_A  has a chainbreak, removing...\n",
      "1UWU_1_A  has a chainbreak, removing...\n",
      "1SBZ_1_A  has a chainbreak, removing...\n",
      "2AHV_1_A  has a chainbreak, removing...\n",
      "1XMC_d1xmcb2  has a chainbreak, removing...\n",
      "4HTC_3_I  has a chainbreak, removing...\n",
      "1P0Y_d1p0yb2  has a chainbreak, removing...\n",
      "1IKN_d1ikna2  has a chainbreak, removing...\n",
      "2AUD_1_A  has a chainbreak, removing...\n",
      "1Q3E_1_A  has a chainbreak, removing...\n",
      "1GO3_d1go3e1  has a chainbreak, removing...\n",
      "1OZH_d1ozhd1  has a chainbreak, removing...\n",
      "1NH2_d1nh2c-  has a chainbreak, removing...\n",
      "1U59_1_A  has a chainbreak, removing...\n",
      "1E8Y_d1e8ya1  has a chainbreak, removing...\n",
      "1E1O_1_A  has a chainbreak, removing...\n",
      "1JMK_1_C  has a chainbreak, removing...\n",
      "2C2A_d2c2aa2  has a chainbreak, removing...\n",
      "1WA9_1_A  has a chainbreak, removing...\n",
      "2F6H_1_X  has a chainbreak, removing...\n",
      "1KO7_d1ko7b2  has a chainbreak, removing...\n",
      "2TOD_1_A  has a chainbreak, removing...\n",
      "2B5I_d2b5id1  has a chainbreak, removing...\n",
      "1SQI_d1sqib2  has a chainbreak, removing...\n",
      "1K1X_1_A  has a chainbreak, removing...\n",
      "2C43_1_A  has a chainbreak, removing...\n",
      "1IK9_1_A  has a chainbreak, removing...\n",
      "1SUJ_1_A  has a chainbreak, removing...\n",
      "2MYS_3_C  has a chainbreak, removing...\n",
      "2AMH_1_A  has a chainbreak, removing...\n",
      "1OFD_1_A  has a chainbreak, removing...\n",
      "1OVA_3_C  has a chainbreak, removing...\n",
      "1PIN_1_A  has a chainbreak, removing...\n",
      "2FOK_d2fokb1  has a chainbreak, removing...\n",
      "1LDJ_1_A  has a chainbreak, removing...\n",
      "1JRR_1_A  has a chainbreak, removing...\n",
      "1WKY_d1wkya1  has a chainbreak, removing...\n",
      "1JPD_1_X  has a chainbreak, removing...\n",
      "2AHX_1_A  has a chainbreak, removing...\n",
      "2CDC_1_A  has a chainbreak, removing...\n",
      "1A21_d1a21b1  has a chainbreak, removing...\n",
      "1B25_1_A  has a chainbreak, removing...\n",
      "1JK0_2_B  has a chainbreak, removing...\n",
      "1ZTM_1_A  has a chainbreak, removing...\n",
      "1O6C_1_A  has a chainbreak, removing...\n",
      "2FNA_d2fnab2  has a chainbreak, removing...\n",
      "1DZB_1_A  has a chainbreak, removing...\n",
      "2ES4_2_D  has a chainbreak, removing...\n",
      "1OIS_1_A  has a chainbreak, removing...\n",
      "1V7W_d1v7wa1  has a chainbreak, removing...\n",
      "1H5W_1_A  has a chainbreak, removing...\n",
      "2F4L_1_A  has a chainbreak, removing...\n",
      "2BG1_1_A  has a chainbreak, removing...\n",
      "1Z84_d1z84b1  has a chainbreak, removing...\n",
      "1V5W_d1v5wb-  has a chainbreak, removing...\n",
      "1F9Z_1_A  has a chainbreak, removing...\n",
      "1U8X_d1u8xx2  has a chainbreak, removing...\n",
      "1B6R_d1b6ra3  has a chainbreak, removing...\n",
      "1KHD_d1khdc2  has a chainbreak, removing...\n",
      "2A5Y_d2a5yc2  has a chainbreak, removing...\n",
      "1FGK_1_A  has a chainbreak, removing...\n",
      "2A91_1_A  has a chainbreak, removing...\n",
      "1M6I_d1m6ia3  has a chainbreak, removing...\n",
      "1TZZ_d1tzzb1  has a chainbreak, removing...\n",
      "2FUG_4_4  has a chainbreak, removing...\n",
      "1NVU_2_S  has a chainbreak, removing...\n",
      "1OLT_1_A  has a chainbreak, removing...\n",
      "1VER_1_A  has a chainbreak, removing...\n",
      "1DX4_1_A  has a chainbreak, removing...\n",
      "1HYH_d1hyhd2  has a chainbreak, removing...\n",
      "1K25_1_A  has a chainbreak, removing...\n",
      "2BVY_d2bvya1  has a chainbreak, removing...\n",
      "1VQP_14_L  has a chainbreak, removing...\n",
      "1H7S_d1h7sa2  has a chainbreak, removing...\n",
      "1GO3_1_E  has a chainbreak, removing...\n",
      "1QKL_1_A  missing structure, removing...\n"
     ]
    }
   ],
   "source": [
    "some_proteins = read_proteinnet_file('/Users/Deathvoodoo/big_folders_msc/data/casp7/training_70')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_mask = []\n",
    "for key in some_proteins.keys():\n",
    "    if not 'mask' in some_proteins[key].keys():\n",
    "        has_mask.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1246"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how big the proteins are, dont wanna pad a million zeroes\n",
    "keylengths=[len(some_proteins[key]['primary']) for key in some_proteins.keys()]\n",
    "max(keylengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(805, 20, 1500)\n",
      "(805, 3, 1499)\n"
     ]
    }
   ],
   "source": [
    "max_length = 1500\n",
    "sequence_list = []\n",
    "angle_list = []\n",
    "for key in some_proteins.keys():\n",
    "    sequence = aa_to_onehot(some_proteins[key]['primary'], aa_id_dict, some_proteins[key]['mask'])\n",
    "    coords = some_proteins[key]['tertiary']\n",
    "    angles = calc_angles(coords).T\n",
    "    if max_length-sequence.shape[0] < 0:\n",
    "        print(key, ' exceeds max length: ', max_length, ' , skipping...')\n",
    "        continue\n",
    "    else:\n",
    "        sequence_padded = np.pad(sequence, pad_width=((0,0), (0, max_length-sequence.shape[1])), constant_values=0)\n",
    "        sequence_list.append(sequence_padded)\n",
    "        angles_padded = np.pad(angles, pad_width=((0,0), (0, max_length-sequence.shape[1])), constant_values=0)\n",
    "        angle_list.append(angles_padded)\n",
    "print(np.array(sequence_list).shape)\n",
    "print(np.array(angle_list).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(sequence_list)\n",
    "have_nans = []\n",
    "for i,j in enumerate(angle_list):\n",
    "    if np.isnan(j).any():\n",
    "        have_nans.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor = torch.tensor(sequence_list).float()\n",
    "sequence_tensor = sequence_tensor.unsqueeze(1)\n",
    "angle_tensor = torch.tensor(angle_list).float()\n",
    "angle_tensor = angle_tensor.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_to_keep = np.logical_not(np.isin(np.arange(N), have_nans))\n",
    "sequence_tensor = sequence_tensor[inds_to_keep, :, :, :]\n",
    "angle_tensor = angle_tensor[inds_to_keep, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_inds(N, train_fraction, validation_fraction):\n",
    "    indices = np.arange(N)\n",
    "    train_to = int(np.floor(len(indices)*train_fraction))\n",
    "    train_inds = indices[0: train_to]\n",
    "    val_to = int(np.floor(len(indices)*(train_fraction+validation_fraction)))\n",
    "    val_inds = indices[train_to: val_to]\n",
    "    test_inds = indices[val_to:]\n",
    "    return(train_inds, val_inds, test_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, val_inds, test_inds = split_inds(angle_tensor.shape[0], 0.8, 0.1)\n",
    "\n",
    "train_data = [sequence_tensor[train_inds, :, :, :], angle_tensor[train_inds, :, :, :]]\n",
    "val_data = [sequence_tensor[val_inds, :, :, :], angle_tensor[val_inds, :, :, :]]\n",
    "test_data = [sequence_tensor[test_inds, :, :, :], angle_tensor[test_inds, :, :, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class proteindataset(Dataset):\n",
    "    def __init__(self, seqs, angles):\n",
    "        self.sequences = seqs\n",
    "        self.angles = angles\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sequences.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.sequences[idx, :, :, :], self.angles[idx, :, :, :]]\n",
    "    \n",
    "class proteindataset2(Dataset): # needs to be fixed\n",
    "    def __init__(self, ids):\n",
    "        self.ids = ids # should be np.array\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, idx):\n",
    "        id_sub = self.ids[idx]\n",
    "        seq_string = seqs[id_sub]['primary']\n",
    "        sequence =  aa_to_onehot(seq_string, aa_id_dict, seqs[id_sub]['mask'])\n",
    "        coords = seqs[id_sub]['tertiary']\n",
    "        angles = calc_angles(coords).T\n",
    "        seq_tensor = torch.tensor(sequence).float().unsqueeze(0)\n",
    "        angle_tensor = torch.tensor(angles).float().unsqueeze(0)\n",
    "        return [seq_tensor.unsqueeze(0), angle_tensor.unsqueeze(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = proteindataset(train_data[0], train_data[1])\n",
    "val_dataset = proteindataset(val_data[0], val_data[1])\n",
    "test_dataset = proteindataset(test_data[0], test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_array = np.array(list(some_proteins.keys()))\n",
    "train_dataset2 = proteindataset2(id_array[train_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader2 = torch.utils.data.DataLoader(train_dataset2, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(1, 3, (20, 1))\n",
    "        self.conv1 = nn.Conv2d(3, 6, (1, 6)) #in, out, kernel size (can be 1 number)\n",
    "        self.conv2 = nn.Conv2d(6, 12, (1, 8))\n",
    "        self.conv3 = nn.Conv2d(12, 24, (1, 10))\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=24, out_channels=12, kernel_size=(1, 10))\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=24, out_channels=6, kernel_size=(1, 8))\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=12, out_channels=1, kernel_size=(3, 5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv0_out = F.relu(self.conv0(x))\n",
    "        conv1_out = F.relu(self.conv1(conv0_out))\n",
    "        conv2_out = F.relu(self.conv2(conv1_out))\n",
    "        conv3_out = F.relu(self.conv3(conv2_out))\n",
    "\n",
    "        deconv1_out = self.deconv1(conv3_out)\n",
    "        deconv2_input = torch.cat((conv2_out, deconv1_out), 1)\n",
    "        deconv2_out = self.deconv2(deconv2_input)\n",
    "        deconv3_input = torch.cat((conv1_out, deconv2_out), 1)\n",
    "        deconv3_out = self.deconv3(deconv3_input)\n",
    "\n",
    "        return deconv3_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "epoch: 0, iteration: 1] loss: 0.6928243935108185\n",
      "epoch: 0, iteration: 3] loss: 0.6498852670192719\n",
      "new best loss, saving..\n",
      "epoch: 0, iteration: 5] loss: 0.6395148038864136\n",
      "new best loss, saving..\n",
      "epoch: 1, iteration: 1] loss: 0.6778537631034851\n",
      "epoch: 1, iteration: 3] loss: 0.6675086319446564\n",
      "epoch: 1, iteration: 5] loss: 0.6085064113140106\n",
      "new best loss, saving..\n",
      "epoch: 2, iteration: 1] loss: 0.5809747278690338\n",
      "new best loss, saving..\n",
      "epoch: 2, iteration: 3] loss: 0.7080845832824707\n",
      "epoch: 2, iteration: 5] loss: 0.6179108619689941\n",
      "epoch: 3, iteration: 1] loss: 0.6288362145423889\n",
      "epoch: 3, iteration: 3] loss: 0.6243439614772797\n",
      "epoch: 3, iteration: 5] loss: 0.6206836402416229\n",
      "epoch: 4, iteration: 1] loss: 0.6198828220367432\n",
      "epoch: 4, iteration: 3] loss: 0.6032708585262299\n",
      "epoch: 4, iteration: 5] loss: 0.5829898118972778\n",
      "epoch: 5, iteration: 1] loss: 0.6351326406002045\n",
      "epoch: 5, iteration: 3] loss: 0.5906074047088623\n",
      "epoch: 5, iteration: 5] loss: 0.5736528635025024\n",
      "new best loss, saving..\n",
      "epoch: 6, iteration: 1] loss: 0.565455436706543\n",
      "new best loss, saving..\n",
      "epoch: 6, iteration: 3] loss: 0.6117890477180481\n",
      "epoch: 6, iteration: 5] loss: 0.6079325973987579\n",
      "epoch: 7, iteration: 1] loss: 0.627845287322998\n",
      "epoch: 7, iteration: 3] loss: 0.5790708661079407\n",
      "epoch: 7, iteration: 5] loss: 0.5682531893253326\n",
      "epoch: 8, iteration: 1] loss: 0.594004362821579\n",
      "epoch: 8, iteration: 3] loss: 0.5725577175617218\n",
      "epoch: 8, iteration: 5] loss: 0.5942921936511993\n",
      "epoch: 9, iteration: 1] loss: 0.586559921503067\n",
      "epoch: 9, iteration: 3] loss: 0.594872236251831\n",
      "epoch: 9, iteration: 5] loss: 0.5959040820598602\n",
      "epoch: 10, iteration: 1] loss: 0.5912528932094574\n",
      "epoch: 10, iteration: 3] loss: 0.5804015696048737\n",
      "epoch: 10, iteration: 5] loss: 0.60161492228508\n",
      "epoch: 11, iteration: 1] loss: 0.5542042553424835\n",
      "new best loss, saving..\n",
      "epoch: 11, iteration: 3] loss: 0.6110084056854248\n",
      "epoch: 11, iteration: 5] loss: 0.6093194782733917\n",
      "epoch: 12, iteration: 1] loss: 0.6031118929386139\n",
      "epoch: 12, iteration: 3] loss: 0.5791606307029724\n",
      "epoch: 12, iteration: 5] loss: 0.5806468427181244\n",
      "epoch: 13, iteration: 1] loss: 0.5880589485168457\n",
      "epoch: 13, iteration: 3] loss: 0.5618800818920135\n",
      "epoch: 13, iteration: 5] loss: 0.6180507838726044\n",
      "epoch: 14, iteration: 1] loss: 0.6144611239433289\n",
      "epoch: 14, iteration: 3] loss: 0.5795662701129913\n",
      "epoch: 14, iteration: 5] loss: 0.5635797083377838\n",
      "epoch: 15, iteration: 1] loss: 0.5654004216194153\n",
      "epoch: 15, iteration: 3] loss: 0.6244276165962219\n",
      "epoch: 15, iteration: 5] loss: 0.5908669829368591\n",
      "epoch: 16, iteration: 1] loss: 0.631060928106308\n",
      "epoch: 16, iteration: 3] loss: 0.5752741396427155\n",
      "epoch: 16, iteration: 5] loss: 0.5684727132320404\n",
      "epoch: 17, iteration: 1] loss: 0.5849275887012482\n",
      "epoch: 17, iteration: 3] loss: 0.5844070911407471\n",
      "epoch: 17, iteration: 5] loss: 0.5890035927295685\n",
      "epoch: 18, iteration: 1] loss: 0.590923011302948\n",
      "epoch: 18, iteration: 3] loss: 0.5356366634368896\n",
      "new best loss, saving..\n",
      "epoch: 18, iteration: 5] loss: 0.6221492886543274\n",
      "epoch: 19, iteration: 1] loss: 0.6467942595481873\n",
      "epoch: 19, iteration: 3] loss: 0.5477037727832794\n",
      "epoch: 19, iteration: 5] loss: 0.5768046975135803\n",
      "epoch: 20, iteration: 1] loss: 0.5867840945720673\n",
      "epoch: 20, iteration: 3] loss: 0.5879532098770142\n",
      "epoch: 20, iteration: 5] loss: 0.5915830135345459\n",
      "epoch: 21, iteration: 1] loss: 0.5797867178916931\n",
      "epoch: 21, iteration: 3] loss: 0.5994191467761993\n",
      "epoch: 21, iteration: 5] loss: 0.5784738063812256\n",
      "epoch: 22, iteration: 1] loss: 0.6481660604476929\n",
      "epoch: 22, iteration: 3] loss: 0.5379182696342468\n",
      "epoch: 22, iteration: 5] loss: 0.5719206035137177\n",
      "epoch: 23, iteration: 1] loss: 0.6006691753864288\n",
      "epoch: 23, iteration: 3] loss: 0.586436927318573\n",
      "epoch: 23, iteration: 5] loss: 0.5648165047168732\n",
      "epoch: 24, iteration: 1] loss: 0.6153731644153595\n",
      "epoch: 24, iteration: 3] loss: 0.5780527591705322\n",
      "epoch: 24, iteration: 5] loss: 0.572803795337677\n",
      "epoch: 25, iteration: 1] loss: 0.6089647114276886\n",
      "epoch: 25, iteration: 3] loss: 0.5479254126548767\n",
      "epoch: 25, iteration: 5] loss: 0.5965592861175537\n",
      "epoch: 26, iteration: 1] loss: 0.5784910917282104\n",
      "epoch: 26, iteration: 3] loss: 0.5806813538074493\n",
      "epoch: 26, iteration: 5] loss: 0.57927006483078\n",
      "epoch: 27, iteration: 1] loss: 0.6422868072986603\n",
      "epoch: 27, iteration: 3] loss: 0.5560740232467651\n",
      "epoch: 27, iteration: 5] loss: 0.5472436845302582\n",
      "epoch: 28, iteration: 1] loss: 0.6044173240661621\n",
      "epoch: 28, iteration: 3] loss: 0.5761608779430389\n",
      "epoch: 28, iteration: 5] loss: 0.5791267454624176\n",
      "epoch: 29, iteration: 1] loss: 0.6202680766582489\n",
      "epoch: 29, iteration: 3] loss: 0.5632889866828918\n",
      "epoch: 29, iteration: 5] loss: 0.5715471804141998\n",
      "epoch: 30, iteration: 1] loss: 0.5787056386470795\n",
      "epoch: 30, iteration: 3] loss: 0.598186582326889\n",
      "epoch: 30, iteration: 5] loss: 0.5515159666538239\n",
      "epoch: 31, iteration: 1] loss: 0.5443773865699768\n",
      "epoch: 31, iteration: 3] loss: 0.5313744843006134\n",
      "new best loss, saving..\n",
      "epoch: 31, iteration: 5] loss: 0.6345112323760986\n",
      "epoch: 32, iteration: 1] loss: 0.5935222208499908\n",
      "epoch: 32, iteration: 3] loss: 0.624968409538269\n",
      "epoch: 32, iteration: 5] loss: 0.5326807498931885\n",
      "epoch: 33, iteration: 1] loss: 0.5615251064300537\n",
      "epoch: 33, iteration: 3] loss: 0.5873706042766571\n",
      "epoch: 33, iteration: 5] loss: 0.5900124311447144\n",
      "epoch: 34, iteration: 1] loss: 0.5488263666629791\n",
      "epoch: 34, iteration: 3] loss: 0.5734775960445404\n",
      "epoch: 34, iteration: 5] loss: 0.5858482420444489\n",
      "epoch: 35, iteration: 1] loss: 0.5574871301651001\n",
      "epoch: 35, iteration: 3] loss: 0.5823586881160736\n",
      "epoch: 35, iteration: 5] loss: 0.5667019784450531\n",
      "epoch: 36, iteration: 1] loss: 0.5422763377428055\n",
      "epoch: 36, iteration: 3] loss: 0.5612314939498901\n",
      "epoch: 36, iteration: 5] loss: 0.5967896282672882\n",
      "epoch: 37, iteration: 1] loss: 0.5409329235553741\n",
      "epoch: 37, iteration: 3] loss: 0.5947189033031464\n",
      "epoch: 37, iteration: 5] loss: 0.5414271056652069\n",
      "epoch: 38, iteration: 1] loss: 0.51059789955616\n",
      "new best loss, saving..\n",
      "epoch: 38, iteration: 3] loss: 0.5874729156494141\n",
      "epoch: 38, iteration: 5] loss: 0.5765719413757324\n",
      "epoch: 39, iteration: 1] loss: 0.5369990170001984\n",
      "epoch: 39, iteration: 3] loss: 0.5961925685405731\n",
      "epoch: 39, iteration: 5] loss: 0.5379335582256317\n",
      "epoch: 40, iteration: 1] loss: 0.5441114902496338\n",
      "epoch: 40, iteration: 3] loss: 0.5562092065811157\n",
      "epoch: 40, iteration: 5] loss: 0.5567499101161957\n",
      "epoch: 41, iteration: 1] loss: 0.5521388053894043\n",
      "epoch: 41, iteration: 3] loss: 0.535959005355835\n",
      "epoch: 41, iteration: 5] loss: 0.51466965675354\n",
      "epoch: 42, iteration: 1] loss: 0.5457456409931183\n",
      "epoch: 42, iteration: 3] loss: 0.5513217151165009\n",
      "epoch: 42, iteration: 5] loss: 0.5173557698726654\n",
      "epoch: 43, iteration: 1] loss: 0.5410335063934326\n",
      "epoch: 43, iteration: 3] loss: 0.5037620961666107\n",
      "new best loss, saving..\n",
      "epoch: 43, iteration: 5] loss: 0.5473317801952362\n",
      "epoch: 44, iteration: 1] loss: 0.5537697672843933\n",
      "epoch: 44, iteration: 3] loss: 0.5284196436405182\n",
      "epoch: 44, iteration: 5] loss: 0.49299490451812744\n",
      "new best loss, saving..\n",
      "epoch: 45, iteration: 1] loss: 0.5131587088108063\n",
      "epoch: 45, iteration: 3] loss: 0.5130811184644699\n",
      "epoch: 45, iteration: 5] loss: 0.5553286671638489\n",
      "epoch: 46, iteration: 1] loss: 0.5073508024215698\n",
      "epoch: 46, iteration: 3] loss: 0.537079855799675\n",
      "epoch: 46, iteration: 5] loss: 0.5191098153591156\n",
      "epoch: 47, iteration: 1] loss: 0.5176371335983276\n",
      "epoch: 47, iteration: 3] loss: 0.5234310925006866\n",
      "epoch: 47, iteration: 5] loss: 0.5212396830320358\n",
      "epoch: 48, iteration: 1] loss: 0.5304480493068695\n",
      "epoch: 48, iteration: 3] loss: 0.5551939606666565\n",
      "epoch: 48, iteration: 5] loss: 0.4665854573249817\n",
      "new best loss, saving..\n",
      "epoch: 49, iteration: 1] loss: 0.5309335887432098\n",
      "epoch: 49, iteration: 3] loss: 0.5401132702827454\n",
      "epoch: 49, iteration: 5] loss: 0.4759451448917389\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=1e-2)\n",
    "\n",
    "prints_per_epoch = 3\n",
    "\n",
    "verbose_k = np.floor(len(trainloader)/prints_per_epoch)\n",
    "print(verbose_k)\n",
    "\n",
    "losses = []\n",
    "iterations = []\n",
    "best_loss = None\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "    #for i in range(52):\n",
    "        #sequence = train_data[0][i, :, :, :].unsqueeze(0)\n",
    "        #true_angles = train_data[1][i, :, :, :].unsqueeze(0)\n",
    "        sequence, true_angles = data\n",
    "        #print(sequence.shape, true_angles.shape)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        predicted_angles = net(sequence)\n",
    "\n",
    "        loss = criterion(predicted_angles, true_angles)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics, should add validation loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % verbose_k == 0:   \n",
    "            losses.append(running_loss/verbose_k)\n",
    "            true_iter = len(trainloader)*epoch + i\n",
    "            iterations.append(true_iter)\n",
    "            \n",
    "            print('epoch: {}, iteration: {}] loss: {}'.format(epoch, i, running_loss/verbose_k))\n",
    "            \n",
    "            if best_loss == None:\n",
    "                best_loss = running_loss/verbose_k\n",
    "            else:\n",
    "                if running_loss/verbose_k <= min(losses):\n",
    "                    print('new best loss, saving..')\n",
    "                    best_loss = running_loss/verbose_k\n",
    "                    torch.save(net.state_dict(), 'best_fcn_parameters.pt')\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6038327217102051\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('/Users/Deathvoodoo/Documents/random/best_fcn_parameters.pt'))\n",
    "\n",
    "net.eval()\n",
    "\n",
    "validation_pred = net(val_data[0])\n",
    "validation_loss = criterion(validation_pred, val_data[1])\n",
    "print(validation_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(iterations, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4665854573249817"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7892)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_pred.detach().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([798, 1, 3, 1499])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
